{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This code was created on Kaggle Kernel for the Petfinder competition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#from tqdm import tqdm, tqdm_notebook\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#import re\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "#import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import json\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Input, GRU, Activation\n",
    "from keras import applications\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout, GRU, Activation, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model,Sequential\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.initializers import glorot_uniform\n",
    "from keras import optimizers \n",
    "import lightgbm as lgb\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "input_meta_dir = '../input/petfinder-adoption-prediction/'\n",
    "train_images_data_dir = '../input/petfinder-adoption-prediction/train_images'\n",
    "test_images_data_dir = '../input/petfinder-adoption-prediction/test_images'\n",
    "#input_meta_dir = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "meta_train = pd.read_csv(input_meta_dir + 'train/train.csv')\n",
    "meta_test = pd.read_csv(input_meta_dir + 'test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_train.shape)\n",
    "print(meta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_dataframe(split_type='train'):\n",
    "    sentiment_split = '{split_type}_sentiment/'.format(split_type=split_type)\n",
    "    train_sentiment = []\n",
    "    for filename in os.listdir(input_meta_dir + sentiment_split):\n",
    "        with open(input_meta_dir + sentiment_split + filename, 'r') as json_file:    \n",
    "            data = json.load(json_file)\n",
    "            info_to_keep = data['documentSentiment'] # e.g. {'magnitude': 2.1, 'score': 0.4}\n",
    "            info_to_keep['Language'] = data['language']\n",
    "            info_to_keep['PetID'] = filename.split('.')[0]\n",
    "            train_sentiment.append(info_to_keep)\n",
    "\n",
    "    train_sentiment_df = pd.DataFrame(train_sentiment)\n",
    "    return train_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = load_sentiment_dataframe()\n",
    "test_sentiment = load_sentiment_dataframe(split_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train = meta_train.merge(train_sentiment, on='PetID', how='left')\n",
    "meta_test = meta_test.merge(test_sentiment, on='PetID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = meta_train.append(meta_test, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_map(label_type):\n",
    "    labels = pd.read_csv(input_meta_dir + label_type.lower() + '_labels.csv')\n",
    "    labels_map = dict(zip(labels['{}ID'.format(label_type)], labels['{}Name'.format(label_type)]))\n",
    "    return labels_map\n",
    "\n",
    "state_map = get_labels_map('State')\n",
    "breed_map = get_labels_map('Breed')\n",
    "color_map = get_labels_map('Color')\n",
    "\n",
    "data.loc[:, 'Type'] = data.Type.map({1:'Dog', 2:'Cat'})\n",
    "data.loc[:, 'Breed1'] = data.Breed1.map(breed_map)\n",
    "data.loc[:, 'Breed2'] = data.Breed2.map(breed_map)\n",
    "data.loc[:, 'Color1'] = data.Color1.map(color_map)\n",
    "data.loc[:, 'Color2'] = data.Color2.map(color_map)\n",
    "data.loc[:, 'Color3'] = data.Color3.map(color_map)\n",
    "data.loc[:, 'State'] = data.State.map(state_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impute_dict(data, var_name, top_n):\n",
    "    impute_dict = {x:x for x in data[var_name].value_counts()[:top_n].index.values}\n",
    "    impute_dict.update({'Missing':'Missing'})\n",
    "    return impute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data.Name.fillna('no name', inplace=True)\n",
    "    data.loc[:, 'NoName'] = data.Name.str.lower().str.contains('no name').astype(int)\n",
    "    data.loc[:, 'NameWordLength'] = data.Name.apply(lambda x: len(x.split(' ')))\n",
    "    data.Description.fillna('None', inplace=True)\n",
    "    data.loc[:, 'NameInDescription'] = data.apply(lambda record: record.Name.lower() in record.Description.lower(), axis=1).astype(int)\n",
    "    data.loc[:, 'DescriptionWordLength'] = data.Description.apply(lambda x: len(x.split(' ')))\n",
    "    #bins = np.array([0, 10, 25, 50, 100, 150, 200])\n",
    "    #data.loc[:, 'DescriptionWordLength'] = np.digitize(data.DescriptionWordLength.values, bins)\n",
    "    #data.loc[:, 'Breed1'] = data.Breed1.fillna('Missing').map(get_impute_dict(data, 'Breed1', 10)).fillna('Other')\n",
    "    #data.loc[:, 'Breed2'] = data.Breed2.fillna('Missing').map(get_impute_dict(data, 'Breed2', 10)).fillna('Other')\n",
    "    data.loc[:, 'Breed1'] = data.Breed1.map(get_impute_dict(data, 'Breed1', 10)).fillna('Other')\n",
    "    data.loc[:, 'Breed2'] = data.Breed2.map(get_impute_dict(data, 'Breed2', 10)).fillna('Other')\n",
    "    data.loc[:, 'RescuerCount'] = data.groupby(['RescuerID'])['Type'].transform('count') \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_data (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for target-encoding - may be done better\n",
    "# Modified to include test set as well\n",
    "\n",
    "def calc_smooth_mean(df, by, on, m):\n",
    "    # df - input pandas dataframe\n",
    "    # Compute the global mean\n",
    "    mean = df[on].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(by)[on].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    #return smooth\n",
    "    return df[by].map(smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TE = target encoding\n",
    "te_weight = 10\n",
    "data['Breed1'] = calc_smooth_mean(data, by='Breed1', on='AdoptionSpeed', m=te_weight)\n",
    "data['Breed2'] = calc_smooth_mean(data, by='Breed2', on='AdoptionSpeed', m=te_weight)\n",
    "data['Color1'] = calc_smooth_mean(data, by='Breed1', on='AdoptionSpeed', m=te_weight)\n",
    "data['Color2'] = calc_smooth_mean(data, by='Breed2', on='AdoptionSpeed', m=te_weight)\n",
    "data['Color3'] = calc_smooth_mean(data, by='Breed1', on='AdoptionSpeed', m=te_weight)\n",
    "data['State'] = calc_smooth_mean(data, by='State', on='AdoptionSpeed', m=te_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "do_not_use = ['Name', 'Description', 'RescuerID'] #, 'PetID'] - do not forget remove it later!\n",
    "categorical_cols = ['Type', 'Language']\n",
    "data = pd.get_dummies(data, columns =categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (do_not_use, axis =1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =data.loc[np.isfinite(data.AdoptionSpeed), :]\n",
    "X_test = data.loc[~np.isfinite(data.AdoptionSpeed), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape[0] == meta_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "print (X_test.shape[0]  == meta_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  IMAGE FEATURES from VGG16 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from the kernel https://www.kaggle.com/mkozine/weighted-kappa-loss-for-keras-tensorflow\n",
    "# Eliminated bsize = (batch size), use y_pred.shape[0] instead\n",
    "\n",
    "def kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, bsize=256, N=5, name='kappa'):\n",
    "    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n",
    "        Args:\n",
    "            y_pred: 2D tensor or array, [batch_size, num_classes]\n",
    "            y_true: 2D tensor or array,[batch_size, num_classes]\n",
    "            y_pow: int,  e.g. y_pow=2\n",
    "            N: typically num_classes of the model\n",
    "                        eps: a float, prevents divide by zero\n",
    "            name: Optional scope/name for op_scope.\n",
    "        Returns:\n",
    "            A tensor with the kappa loss.\"\"\"\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom / (denom + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_target_dict = meta_train.set_index('PetID')['AdoptionSpeed'].to_dict()\n",
    "\n",
    "train_image_names = os.listdir(train_images_data_dir)\n",
    "n_train_images = len(train_image_names)\n",
    "test_image_names = os.listdir(test_images_data_dir)\n",
    "n_test_images = len(test_image_names)\n",
    "print (train_image_names [0:2])\n",
    "print (\"No. of train images: \" + str (n_train_images))\n",
    "print (\"No. of test images: \" + str (n_test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_dict = {'filename': [], 'PetID':[], 'class': []}\n",
    "\n",
    "for name in train_image_names:\n",
    "    short_name = name.split('-')[0]\n",
    "    label = name_target_dict[short_name]\n",
    "    \n",
    "    generator_dict['filename'].append(name)\n",
    "    generator_dict['PetID'].append(short_name)\n",
    "    generator_dict['class'].append(label)\n",
    "\n",
    "generator_df_full = pd.DataFrame(generator_dict)\n",
    "print (generator_df_full.shape)\n",
    "generator_df_full[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_target_dict = meta_test.set_index('PetID').to_dict()\n",
    "test_generator_dict = {'filename': [], 'PetID':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in test_image_names:\n",
    "    short_name = name.split('-')[0]\n",
    "    #label = test_name_target_dict[short_name]\n",
    "    \n",
    "    test_generator_dict['filename'].append(name)\n",
    "    test_generator_dict['PetID'].append(short_name)\n",
    "    \n",
    "\n",
    "test_generator_df = pd.DataFrame(test_generator_dict)\n",
    "test_generator_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=6)\n",
    "mask = np.random.randn(len(meta_train)) < 0.9\n",
    "train_split = X_train[mask]\n",
    "validation_split = X_train[~mask]\n",
    "print (train_split.shape)\n",
    "print (validation_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dicts will be helpful to attach the image features to the right PetID\n",
    "train_generator_full_df = generator_df_full.loc[generator_df_full['PetID'].isin(train_split['PetID'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_df = train_generator_full_df.copy()\n",
    "train_generator_df = train_generator_df[['filename','class']]\n",
    "train_generator_df['class'] = train_generator_df['class'].astype(str)\n",
    "train_generator_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator_full_df = generator_df_full.loc[generator_df_full['PetID'].isin(validation_split['PetID'].values)]\n",
    "valid_generator_df = valid_generator_full_df.copy()\n",
    "valid_generator_df = valid_generator_df[['filename','class']]\n",
    "valid_generator_df['class'] = valid_generator_df['class'].astype(str)\n",
    "valid_generator_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator( rescale=1/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generator for the VGG16 part of the model\n",
    "def create_generator_vgg16(data_dir, input_df):\n",
    "    return datagen.flow_from_dataframe(\n",
    "        input_df, \n",
    "        data_dir, \n",
    "        x_col='filename',\n",
    "        y_col='class', \n",
    "        has_ext=True,  # If image extension is given in x_col\n",
    "        target_size=(img_width, img_height), \n",
    "        color_mode='rgb',\n",
    "        class_mode=None, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, # we will just apply the fixed VGG16 weights to the images \n",
    "        seed=6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make length of train and valid splits divisible by batch_size\n",
    "# for the future VGG-16 propagation needs\n",
    "n_train_split = train_generator_df.shape[0]//batch_size*batch_size\n",
    "print (\" Length of train portion decreased from \" + str(train_generator_df.shape[0]) + \" to \" \n",
    "       + str (n_train_split))\n",
    "n_valid_split = valid_generator_df.shape[0]//batch_size*batch_size\n",
    "print (\" Length of valid portion decreased from \" + str(valid_generator_df.shape[0]) + \" to \" \n",
    "       + str (n_valid_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_df_short = train_generator_df.head(n_train_split)\n",
    "train_generator_full_df_short = train_generator_full_df.head(n_train_split)\n",
    "train_generator_df_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator_full_df_short = valid_generator_full_df.head(n_valid_split)\n",
    "valid_generator_df_short = valid_generator_df.head(n_valid_split)\n",
    "valid_generator_df_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = create_generator_vgg16(train_images_data_dir, train_generator_df_short)\n",
    "valid_generator = create_generator_vgg16(train_images_data_dir, valid_generator_df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "base_model = VGG16(include_top = False,\n",
    "                  input_shape=(img_width, img_height,3),\n",
    "                  weights='../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = base_model.predict_generator(train_generator, n_train_split//batch_size, verbose = 1)\n",
    "bottleneck_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_valid = base_model.predict_generator(valid_generator, n_valid_split // batch_size, verbose = 1)\n",
    "bottleneck_features_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_labels = to_categorical(train_generator_df_short['class'])\n",
    "valid_labels = to_categorical(valid_generator_df_short['class'])\n",
    "valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dense layesr model on to of VGG16\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.6))\n",
    "top_model.add(BatchNormalization (epsilon=0.001))\n",
    "#top_model.add(Dense(16, activation='relu'))\n",
    "#top_model.add(Dropout(0.4))\n",
    "#top_model.add(BatchNormalization (epsilon=0.001))\n",
    "top_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_model.compile(optimizer='rmsprop',\n",
    "#              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mko_optimizer = keras.optimizers.rmsprop(lr=0.00005)\n",
    "top_model.compile(loss = kappa_loss,\n",
    "              optimizer = mko_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.fit(bottleneck_features_train, train_labels,\n",
    "          epochs=30,\n",
    "          batch_size=32,\n",
    "          validation_data=(bottleneck_features_valid, valid_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is ready.\n",
    "# Use it to create image features for train, valid and test portions\n",
    "# First - train\n",
    "train_images_predictions = top_model.predict(bottleneck_features_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_full_df_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.DataFrame(train_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\"))\n",
    "tr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack it nicely and merge\n",
    "train_images_df = train_generator_full_df_short.join(pd.DataFrame(train_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\")))\n",
    "train_images_df.drop(columns=['filename', 'class'], inplace=True)\n",
    "train_images_df.loc[:,'Img_pred'] = train_images_df.iloc[:,1:6].values.argmax(axis=1)\n",
    "print(train_images_df.shape)\n",
    "train_images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_df = train_images_df.groupby('PetID', as_index=False).median()\n",
    "train_images_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images_df = train_images_df.groupby('PetID', as_index=False).mean()\n",
    "train_split = pd.merge(train_split, train_images_df, how='left', on = 'PetID')\n",
    "train_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split['Img_pred'][train_split['Img_pred']==0.000000] = float('nan')\n",
    "#train_split['Img_pred'] = train_split.Img_pred.fillna(3)\n",
    "train_split[['AdoptionSpeed','Img_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid\n",
    "valid_images_predictions = top_model.predict(bottleneck_features_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images_df = valid_generator_full_df_short.join(pd.DataFrame(valid_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\")))\n",
    "valid_images_df.drop(columns=['filename', 'class'], inplace=True)\n",
    "valid_images_df['Img_pred'] = valid_images_df.iloc[:,1:6].values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images_df = valid_images_df.groupby('PetID', as_index=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = pd.merge(validation_split, valid_images_df, how='left', on='PetID')\n",
    "print (validation_split.shape)\n",
    "validation_split.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split['Img_pred'][validation_split['Img_pred']==0.000000] = float ('nan')\n",
    "#validation_split['Img_pred'] = validation_split.Img_pred.fillna(3)\n",
    "#validation_split['Img_0'] = validation_split.Img_0.fillna(0)\n",
    "#validation_split['Img_1'] = validation_split.Img_1.fillna(0)\n",
    "#validation_split['Img_2'] = validation_split.Img_2.fillna(0)\n",
    "#validation_split['Img_3'] = validation_split.Img_3.fillna(0)\n",
    "#validation_split['Img_4'] = validation_split.Img_4.fillna(0)\n",
    "#validation_split[['AdoptionSpeed','Img_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = train_split.append(validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_generator = ImageDataGenerator(rescale=1/255.).flow_from_dataframe(\n",
    "    test_generator_df,\n",
    "    test_images_data_dir,\n",
    "    has_ext=True,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode='rgb',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    class_mode=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_test = base_model.predict_generator(test_generator, len(test_generator), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_predictions = top_model.predict(bottleneck_features_test, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_df = test_generator_df.join(pd.DataFrame(test_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\")))\n",
    "test_images_df.drop(columns=['filename'], inplace=True)\n",
    "print(test_images_df.shape)\n",
    "test_images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[:, 'NameWordLength'] =\n",
    "test_images_df.loc[:,'Img_pred'] = test_images_df.iloc[:,1:6].values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_df = test_images_df.groupby('PetID', as_index=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_test.shape)\n",
    "X_test = pd.merge(X_test, test_images_df, how='left', on = 'PetID')\n",
    "print (X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Img_pred'][X_test['Img_pred']==0.000000] = float('nan')\n",
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### End IMAGE FEATURES from VGG16 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VGG16 garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [train_generator_df, valid_generator_df, test_generator_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [bottleneck_features_train, bottleneck_features_valid, bottleneck_features_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [train_generator, valid_generator, test_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [train_generator_full_df, valid_generator_full_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [base_model, top_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [train_images_df, valid_images_df, test_images_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Done with VGG16 ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop('PetID', axis = 1)\n",
    "X_train1 = X_train1.drop('PetID', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_non_null = X_train1.fillna(-1)\n",
    "X_train_non_null = X_train1.fillna(-1)\n",
    "X_test_non_null = X_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_null.shape, X_test_non_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic Kappa calculation\n",
    "\n",
    "# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n",
    "\n",
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal split thresholds\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "    \n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "        return -cohen_kappa_score(y, preds, weights='quadratic')\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X = X, y = y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "    \n",
    "    def predict(self, X, coef):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "        return preds\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "xgb_params = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 6,\n",
    "    'eta':  0.001,\n",
    "    'gamma': 2,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'silent': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(params, X_train, X_test):\n",
    "    n_splits = 10\n",
    "    verbose_eval = 1000\n",
    "    num_rounds = 60000\n",
    "    early_stop = 800\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n",
    "\n",
    "    oof_train = np.zeros((X_train.shape[0]))\n",
    "    oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "\n",
    "        X_tr = X_train.iloc[train_idx, :]\n",
    "        X_val = X_train.iloc[valid_idx, :]\n",
    "\n",
    "        y_tr = X_tr['AdoptionSpeed'].values\n",
    "        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "        y_val = X_val['AdoptionSpeed'].values\n",
    "        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n",
    "        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n",
    "                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n",
    "\n",
    "        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n",
    "        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        oof_train[valid_idx] = valid_pred\n",
    "        oof_test[:, i] = test_pred\n",
    "\n",
    "        i += 1\n",
    "    return model, oof_train, oof_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, X_train1['AdoptionSpeed'].values)\n",
    "coefficients = optR.coefficients()\n",
    "valid_pred = optR.predict(oof_train, coefficients)\n",
    "qwk = quadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, valid_pred)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_ = coefficients.copy()\n",
    "coefficients_[0] = 1.66\n",
    "coefficients_[1] = 2.13\n",
    "coefficients_[3] = 2.85\n",
    "train_predictions = optR.predict(oof_train, coefficients_).astype(np.int8)\n",
    "print(f'train pred distribution: {Counter(train_predictions)}')\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients_).astype(np.int8)\n",
    "print(f'test pred distribution: {Counter(test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = pd.cut(train_predictions, [-np.inf] + list(np.sort(coefficients_)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "quadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LGBM parameters   ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.001,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.9,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17,\n",
    "          'early_stop': 600,\n",
    "          'verbose_eval': 1000,\n",
    "          'num_rounds': 60000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(params, X_train, X_test):\n",
    "    n_splits = 10\n",
    "    verbose_eval = 1000\n",
    "    num_rounds = 60000\n",
    "    early_stop = 800\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n",
    "\n",
    "    oof_train = np.zeros((X_train.shape[0]))\n",
    "    oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "\n",
    "        X_tr = X_train.iloc[train_idx, :]\n",
    "        X_val = X_train.iloc[valid_idx, :]\n",
    "\n",
    "        y_tr = X_tr['AdoptionSpeed'].values\n",
    "        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "        y_val = X_val['AdoptionSpeed'].values\n",
    "        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "        #d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n",
    "        #d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n",
    "        \n",
    "        print('Prep LGB')\n",
    "        d_train = lgb.Dataset(X_tr,  label=y_tr)\n",
    "        d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "        watchlist = [d_train, d_valid]\n",
    "        print('Train LGB')\n",
    "        num_rounds = 10000\n",
    "        verbose_eval = 500\n",
    "        early_stop = 600\n",
    "\n",
    "        model = lgb.train(lgb_params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      #categorical_feature=list(cat_features),\n",
    "                      early_stopping_rounds=early_stop)\n",
    "        #model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n",
    "        #                 early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n",
    "\n",
    "        valid_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "        oof_train[valid_idx] = valid_pred\n",
    "        oof_test[:, i] = test_pred\n",
    "\n",
    "        i += 1\n",
    "    return model, oof_train, oof_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model, lgb_oof_train, lgb_oof_test = run_lgb(lgb_params, X_train_non_null, X_test_non_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR.fit(lgb_oof_train, X_train1['AdoptionSpeed'].values)\n",
    "lgb_coefficients = optR.coefficients()\n",
    "valid_pred = optR.predict(lgb_oof_train, lgb_coefficients)\n",
    "qwk = quadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, valid_pred)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_coefficients_ = lgb_coefficients.copy()\n",
    "lgb_coefficients_[0] = 1.66\n",
    "lgb_coefficients_[1] = 2.13\n",
    "lgb_coefficients_[3] = 2.85\n",
    "lgb_train_predictions = optR.predict(lgb_oof_train, lgb_coefficients_).astype(np.int8)\n",
    "print(f'Lgb train pred distribution: {Counter(train_predictions)}')\n",
    "lgb_test_predictions = optR.predict(lgb_oof_test.mean(axis=1), lgb_coefficients_).astype(np.int8)\n",
    "print(f'Lgb test pred distribution: {Counter(test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = pd.cut(lgb_train_predictions, [-np.inf] + list(np.sort(lgb_coefficients_)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "quadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_ensemble = (oof_test + lgb_oof_test)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_ensemble[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_ensemble = (oof_test + lgb_oof_test)/2.0\n",
    "coefficients_[2] = (coefficients[2] + lgb_coefficients[2]) / 2.0\n",
    "test_pred = optR.predict(oof_ensemble.mean(axis=1), coefficients_).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "#test_PetID = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv').PetID\n",
    "#submission = pd.DataFrame({'PetID':test_PetID, 'AdoptionSpeed':test_predictions})\n",
    "submission = pd.DataFrame({'PetID': meta_test['PetID'].values, 'AdoptionSpeed': test_pred})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "mp = plot_importance(model)\n",
    "fig = mp.figure\n",
    "fig.set_size_inches(15,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "mp = plot_importance(lgb_model)\n",
    "fig = mp.figure\n",
    "fig.set_size_inches(15,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
