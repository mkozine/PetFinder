{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove6b50dtxt', 'petfinder-adoption-prediction']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model,Sequential\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, GRU, Activation, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import optimizers \n",
    "np.random.seed(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desc_df = data_df[['AdoptionSpeed', \"Description\"]]\n",
    "data_desc_df = data_desc_df[data_desc_df['Description'] != \"\"]\n",
    "data_descriptions = data_desc_df['Description'].apply(lambda x: re.sub(r\"[^a-z0-9 ]+\", \"\", str(x).lower()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 24)\n",
      "(14993, 2)\n",
      "14993\n"
     ]
    }
   ],
   "source": [
    "print (data_df.shape)\n",
    "print (data_desc_df.shape)\n",
    "print (len(data_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from DLNg to read in the Glove weights:\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words, words_to_index, index_to_words, word_to_vec_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/glove6b50dtxt/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"worm\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of yielded in the vocabulary is 393810\n",
      "the 400000th word in the vocabulary is ï¿¥\n",
      "[ 4.1846e-01  4.1030e-01  2.0033e-01  3.6384e-01  6.7866e-01  1.6747e-01\n",
      " -4.8366e-01  3.7455e-01 -2.3350e-01  1.0087e+00 -7.6460e-01 -4.9268e-01\n",
      "  1.3099e-01 -2.2862e-01  4.0837e-01 -4.0880e-01  2.9919e-02 -8.7655e-01\n",
      " -2.2908e-01  2.0841e-01  2.1926e-01 -1.5913e-01  6.9237e-01 -1.3015e+00\n",
      "  3.4532e-01  1.3344e-01 -3.4478e-02 -1.7596e-03  3.5787e-01  1.0180e-01\n",
      "  1.8439e+00 -7.9155e-01  1.9466e-01  1.1955e-01  1.0515e+00 -6.4883e-02\n",
      " -2.1768e-01  8.3643e-01 -3.5975e-01 -2.8557e-01 -5.8929e-01 -7.0715e-01\n",
      "  1.3815e-01  1.5321e-01 -8.9717e-02 -2.8388e-01  3.8445e-01  1.2361e+00\n",
      "  2.4727e-02 -7.1862e-01]\n",
      "##\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "word = \"yielded\"\n",
    "index = 400000\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print(word_to_vec_map[word])\n",
    "print(index_to_word[10])\n",
    "word_to_vec_map[index_to_word[10]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxLen = len(max(data_descriptions, key=len).split())\n",
    "maxLen = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, vocab, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. We will cut the sentence short at max_len. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    X_indices = np.zeros ((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words =X[i].lower().split()[:maxLen]\n",
    "        j = 0\n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            if w in vocab:\n",
    "                X_indices[i, j] = word_to_index [w]\n",
    "                j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros ((vocab_len, emb_dim))\n",
    "   \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. \n",
    "    embedding_layer = Embedding (vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model (input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input (shape=input_shape, dtype = \"int32\")\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = GRU(128, return_sequences=True) (embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout (0.5) (X)\n",
    "    X = BatchNormalization()(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = GRU(128) (X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout (0.5) (X)\n",
    "    X = BatchNormalization()(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense (5, activation = \"softmax\" ) (X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation (\"softmax\") (X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model (inputs = sentence_indices, outputs = X)\n",
    "       \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 15, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 15, 128)           68736     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,169,143\n",
      "Trainable params: 168,581\n",
      "Non-trainable params: 20,000,562\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = build_model ((maxLen,), word_to_vec_map, word_to_index)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, bsize=256, N=5, name='kappa'):\n",
    "    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n",
    "        Args:\n",
    "            y_pred: 2D tensor or array, [batch_size, num_classes]\n",
    "            y_true: 2D tensor or array,[batch_size, num_classes]\n",
    "            y_pow: int,  e.g. y_pow=2\n",
    "            N: typically num_classes of the model\n",
    "                        eps: a float, prevents divide by zero\n",
    "            name: Optional scope/name for op_scope.\n",
    "        Returns:\n",
    "            A tensor with the kappa loss.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom / (denom + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description_indices = sentences_to_indices(data_descriptions, vocab, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train valid split\n",
    "np.random.seed(seed=6)\n",
    "mask = np.random.randn(len(data_desc_df)) < 0.9\n",
    "train_desc_df = data_desc_df[mask]\n",
    "valid_desc_df = data_desc_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = data_description_indices[mask]\n",
    "valid_indices = data_description_indices[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = data_descriptions[mask]\n",
    "train_labels = train_desc_df['AdoptionSpeed']\n",
    "\n",
    "valid_descriptions = data_descriptions[~mask]\n",
    "valid_labels = valid_desc_df['AdoptionSpeed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten train for faster learning - only on tune-up stage !!\n",
    "#train_indices = train_indices[:3000]\n",
    "#train_labels = train_labels[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To one-hot representation\n",
    "train_labels_oh = to_categorical(train_labels)\n",
    "valid_labels_oh = to_categorical(valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr1 = train_indices[:1000]\\ntr1_labels_oh = train_labels_oh[:1000]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tr1 = train_indices[:1000]\n",
    "tr1_labels_oh = train_labels_oh[:1000]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels_oh[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate (my_lr, my_epochs, my_batch_size):  \n",
    "    mko_optimizer = optimizers.rmsprop(lr = my_lr) \n",
    "    #mko_optimizer = optimizers.Adam(lr = my_lr)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer=mko_optimizer, metrics=['accuracy'])\n",
    "    gru_model.compile(loss=kappa_loss, optimizer=mko_optimizer, metrics=['accuracy'])\n",
    "    #model.fit(tr1, tr1_labels_oh, epochs = my_epochs, batch_size = my_batch_size, verbose = 0, shuffle=True)\n",
    "    gru_model.fit(train_indices, train_labels_oh, epochs = my_epochs, batch_size = my_batch_size, verbose = 0, shuffle=True)\n",
    "    #valid_pred = model.predict (valid_indices)\n",
    "    v_df = pd.DataFrame(model.predict (valid_indices))\n",
    "    v = v_df.values.argmax(axis=1)\n",
    "    return quadratic_weighted_kappa(valid_labels, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for my_epochs in (3,5,10 ):\\n    for my_batch_size in (32,64):\\n        for my_lr in (0.0005, 0.0001):\\n            qwk = model_evaluate (my_lr, my_epochs, 64)\\n            print (\"Epochs = \", str(my_epochs), \" lr = \", str(my_lr), \"batch_size = \", str(my_batch_size), \" qwk = \", str(qwk))        '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''for my_epochs in (3,5,10 ):\n",
    "    for my_batch_size in (32,64):\n",
    "        for my_lr in (0.0005, 0.0001):\n",
    "            qwk = model_evaluate (my_lr, my_epochs, 64)\n",
    "            print (\"Epochs = \", str(my_epochs), \" lr = \", str(my_lr), \"batch_size = \", str(my_batch_size), \" qwk = \", str(qwk))        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for my_epochs in (30,100 ):\\n    for my_batch_size in (16,32,64):\\n        for my_lr in (0.0005, 0.0001):\\n            qwk = model_evaluate (my_lr, my_epochs, 64)\\n            print (\"Epochs = \", str(my_epochs), \" lr = \", str(my_lr), \"batch_size = \", str(my_batch_size), \" qwk = \", str(qwk))        '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for my_epochs in (30,100 ):\n",
    "    for my_batch_size in (16,32,64):\n",
    "        for my_lr in (0.0005, 0.0001):\n",
    "            qwk = model_evaluate (my_lr, my_epochs, 64)\n",
    "            print (\"Epochs = \", str(my_epochs), \" lr = \", str(my_lr), \"batch_size = \", str(my_batch_size), \" qwk = \", str(qwk))        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mko_optimizer = optimizers.rmsprop(lr = 0.0005)\\n#model.compile(loss='categorical_crossentropy', optimizer=mko_optimizer, metrics=['accuracy'])\\nmodel.compile(loss=kappa_loss, optimizer=mko_optimizer, metrics=['accuracy'])\\nmodel.fit(train_indices, train_labels_oh, epochs = 100, batch_size = 16, shuffle=True)\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mko_optimizer = optimizers.rmsprop(lr = 0.0005)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=mko_optimizer, metrics=['accuracy'])\n",
    "model.compile(loss=kappa_loss, optimizer=mko_optimizer, metrics=['accuracy'])\n",
    "model.fit(train_indices, train_labels_oh, epochs = 100, batch_size = 16, shuffle=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v_df = pd.DataFrame(model.predict (valid_indices))\\nv = v_df.values.argmax(axis=1)\\nprint (quadratic_weighted_kappa(valid_labels, v))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''v_df = pd.DataFrame(model.predict (valid_indices))\n",
    "v = v_df.values.argmax(axis=1)\n",
    "print (quadratic_weighted_kappa(valid_labels, v))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs_GRU = 100\n",
    "Batch_size_GRU = 16\n",
    "Lr_GRU = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels_oh = to_categorical (data_desc_df['AdoptionSpeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-852fd70dc620>:13: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "14993/14993 [==============================] - 36s 2ms/step - loss: 15.9927 - acc: 0.2038\n",
      "Epoch 2/100\n",
      "14993/14993 [==============================] - 34s 2ms/step - loss: 15.9420 - acc: 0.2038\n",
      "Epoch 3/100\n",
      "14993/14993 [==============================] - 35s 2ms/step - loss: 15.8815 - acc: 0.2003\n",
      "Epoch 4/100\n",
      "14993/14993 [==============================] - 35s 2ms/step - loss: 15.7954 - acc: 0.1894\n",
      "Epoch 5/100\n",
      "14993/14993 [==============================] - 34s 2ms/step - loss: 15.7014 - acc: 0.1870\n",
      "Epoch 6/100\n",
      "14993/14993 [==============================] - 34s 2ms/step - loss: 15.6324 - acc: 0.1883\n",
      "Epoch 7/100\n",
      "14993/14993 [==============================] - 35s 2ms/step - loss: 15.5900 - acc: 0.1886\n",
      "Epoch 8/100\n",
      "14993/14993 [==============================] - 33s 2ms/step - loss: 15.5466 - acc: 0.1967\n",
      "Epoch 9/100\n",
      "14993/14993 [==============================] - 33s 2ms/step - loss: 15.5443 - acc: 0.1958\n",
      "Epoch 10/100\n",
      "14993/14993 [==============================] - 34s 2ms/step - loss: 15.4279 - acc: 0.1976\n",
      "Epoch 11/100\n",
      "14993/14993 [==============================] - 33s 2ms/step - loss: 15.4666 - acc: 0.1906\n",
      "Epoch 12/100\n",
      "14993/14993 [==============================] - 34s 2ms/step - loss: 15.4393 - acc: 0.1936\n",
      "Epoch 13/100\n",
      "14993/14993 [==============================] - 34s 2ms/step - loss: 15.3879 - acc: 0.1934\n",
      "Epoch 14/100\n",
      " 8272/14993 [===============>..............] - ETA: 14s - loss: 15.3964 - acc: 0.1956"
     ]
    }
   ],
   "source": [
    "#Create the model on the whole text corpus with the best fine-tuned parameters\n",
    "mko_optimizer = optimizers.rmsprop(lr = Lr_GRU)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=mko_optimizer, metrics=['accuracy'])\n",
    "gru_model.compile(loss=kappa_loss, optimizer=mko_optimizer, metrics=['accuracy'])\n",
    "gru_model.fit(data_description_indices, data_labels_oh, epochs = N_epochs_GRU, batch_size = Batch_size_GRU, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc_df = test_df[[\"Description\"]]\n",
    "test_desc_df = test_desc_df[test_desc_df['Description'] != \"\"]\n",
    "test_descriptions = test_desc_df['Description'].apply(lambda x: re.sub(r\"[^a-z0-9 ]+\", \"\", str(x).lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_description_indices = sentences_to_indices(test_descriptions, vocab, word_to_index, maxLen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gru_model.predict(test_description_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14884758, 0.14884758, 0.14884758, 0.14884758, 0.40460968],\n",
       "       [0.14884758, 0.14884758, 0.14884758, 0.14884758, 0.40460968],\n",
       "       [0.14884806, 0.14884819, 0.14884844, 0.14884937, 0.4046059 ],\n",
       "       [0.40444732, 0.14889392, 0.14890654, 0.14888377, 0.14886841],\n",
       "       [0.14884764, 0.14884767, 0.1488477 , 0.14884777, 0.4046092 ],\n",
       "       [0.40427053, 0.14901376, 0.14891756, 0.14889446, 0.14890371],\n",
       "       [0.14892071, 0.14895102, 0.14897238, 0.14910308, 0.40405288],\n",
       "       [0.14884764, 0.14884768, 0.1488477 , 0.14884783, 0.40460908],\n",
       "       [0.14893366, 0.14897655, 0.14899762, 0.14906839, 0.40402383],\n",
       "       [0.1488476 , 0.1488476 , 0.1488476 , 0.1488476 , 0.40460962],\n",
       "       [0.14884758, 0.14884758, 0.14884758, 0.14884758, 0.40460968],\n",
       "       [0.14884764, 0.14884768, 0.14884776, 0.14884767, 0.40460923]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[6:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3948, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              0\n",
       "1  73c10e136              0\n",
       "2  72000c4c5              0\n",
       "3  e147a4b9f              0\n",
       "4  43fbba852              0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Submission\n",
    "sample_submission_df = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')\n",
    "print(sample_submission_df.shape)\n",
    "sample_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 4, 4, 4, 4, 4, 4, 4, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.DataFrame(pred).values.argmax(axis=1)\n",
    "#submission = np.concatenate(sample_submission_df, pd.DataFrame(pred), axis=1 )\n",
    "p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3948, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              0\n",
       "1  73c10e136              4\n",
       "2  72000c4c5              4\n",
       "3  e147a4b9f              4\n",
       "4  43fbba852              4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df['AdoptionSpeed'] = p\n",
    "#submission_df.drop(columns=[0,1,2,3,4], inplace=True)\n",
    "print(sample_submission_df.shape)\n",
    "sample_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df['AdoptionSpeed'] = sample_submission_df['AdoptionSpeed'].fillna(3)\n",
    "sample_submission_df['AdoptionSpeed'] = sample_submission_df['AdoptionSpeed'].astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
